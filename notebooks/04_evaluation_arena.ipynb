{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ‚öîÔ∏è Evaluation Arena: The Showdown\n",
                "\n",
                "This notebook analyzes the performance of **The Intern** (Fine-Tuned LLM) vs **The Librarian** (RAG System).\n",
                "\n",
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import json\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from IPython.display import display, Image\n",
                "\n",
                "# Set style\n",
                "sns.set_theme(style=\"whitegrid\")\n",
                "plt.rcParams['figure.figsize'] = [10, 6]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Results\n",
                "\n",
                "Loading evaluation data generated by `utils/evaluate_systems.py`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    results_df = pd.read_csv('../outputs/evaluation_results.csv')\n",
                "    with open('../outputs/evaluation_summary.json', 'r') as f:\n",
                "        summary = json.load(f)\n",
                "    \n",
                "    print(f\"‚úì Loaded results for {len(results_df)} questions\")\n",
                "    display(results_df.head())\n",
                "except FileNotFoundError:\n",
                "    print(\"‚ùå Results not found! Run utils/evaluate_systems.py first.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. ROUGE Score Comparison\n",
                "\n",
                "Evaluating textual overlap with ground truth answers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üèÜ Winner:\", summary['winner'])\n",
                "print(f\"Improvement: {summary['improvement_percentage']:.1f}%\")\n",
                "\n",
                "# Visualize ROUGE scores\n",
                "metrics = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
                "intern_scores = [summary['the_intern']['rouge_scores'][m] for m in metrics]\n",
                "librarian_scores = [summary['the_librarian']['rouge_scores'][m] for m in metrics]\n",
                "\n",
                "x = range(len(metrics))\n",
                "width = 0.35\n",
                "\n",
                "fig, ax = plt.subplots()\n",
                "rects1 = ax.bar([i - width/2 for i in x], intern_scores, width, label='The Intern')\n",
                "rects2 = ax.bar([i + width/2 for i in x], librarian_scores, width, label='The Librarian')\n",
                "\n",
                "ax.set_ylabel('Score')\n",
                "ax.set_title('ROUGE Score Comparison')\n",
                "ax.set_xticks(x)\n",
                "ax.set_xticklabels(metrics)\n",
                "ax.legend()\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Latency Analysis\n",
                "\n",
                "Comparing generation speed (milliseconds)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "latencies = [summary['the_intern']['avg_latency_ms'], summary['the_librarian']['avg_latency_ms']]\n",
                "models = ['The Intern', 'The Librarian']\n",
                "\n",
                "plt.figure(figsize=(8, 5))\n",
                "sns.barplot(x=models, y=latencies, palette=['#4c72b0', '#55a868'])\n",
                "plt.title('Average Latency per Question')\n",
                "plt.ylabel('Time (ms)')\n",
                "plt.show()\n",
                "\n",
                "print(f\"The Intern Speed: {latencies[0]:.0f} ms\")\n",
                "print(f\"The Librarian Speed: {latencies[1]:.0f} ms\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Qualitative Analysis\n",
                "\n",
                "Let's look at some examples where the models differed significantly."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample random comparison\n",
                "sample = results_df.sample(1).iloc[0]\n",
                "\n",
                "print(f\"‚ùì Question: {sample['Question']}\\n\")\n",
                "print(f\"üìù Ground Truth: {sample['Ground Truth']}\\n\")\n",
                "print(f\"ü§ñ The Intern: {sample['Intern Answer']}\\n\")\n",
                "print(f\"üìö The Librarian: {sample['Librarian Answer']}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}