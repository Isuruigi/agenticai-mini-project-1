{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ü§ñ Operation Ledger-Mind: Fine-Tuning on Google Colab\n",
                "\n",
                "This notebook fine-tunes Llama-3 8B on your Q&A dataset using a **FREE** T4 GPU.\n",
                "\n",
                "## Setup Instructions:\n",
                "1. Go to **Runtime ‚Üí Change runtime type ‚Üí T4 GPU**\n",
                "2. Upload your dataset files to Colab\n",
                "3. Run all cells\n",
                "4. Download the trained model adapters\n",
                "\n",
                "**Expected Time**: 1.5-2 hours"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m557.0/557.0 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
                        "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m540.5/540.5 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25h"
                    ]
                }
            ],
            "source": [
                "# 1. Install dependencies\n",
                "!pip install -q transformers datasets peft bitsandbytes accelerate trl"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "/bin/bash: line 1: nvidia-smi: command not found\n"
                    ]
                }
            ],
            "source": [
                "# 2. Check GPU\n",
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úì Token set\n"
                    ]
                }
            ],
            "source": [
                "# 3. Upload your Hugging Face token\n",
                "from google.colab import userdata\n",
                "import os\n",
                "\n",
                "# Option 1: Set as Colab secret (Recommended)\n",
                "# Go to: üîë (left sidebar) ‚Üí Add HF_TOKEN\n",
                "try:\n",
                "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
                "except:\n",
                "    # Option 2: Paste directly (less secure)\n",
                "    HF_TOKEN = input(\"Enter your Hugging Face token: \")\n",
                "\n",
                "os.environ['HF_TOKEN'] = HF_TOKEN\n",
                "print(\"‚úì Token set\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Upload train.jsonl and golden_test_set.jsonl from your datasets/ folder\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "     <input type=\"file\" id=\"files-e7d612b1-cba2-45b5-af84-d42d939c639c\" name=\"files[]\" multiple disabled\n",
                            "        style=\"border:none\" />\n",
                            "     <output id=\"result-e7d612b1-cba2-45b5-af84-d42d939c639c\">\n",
                            "      Upload widget is only available when the cell has been executed in the\n",
                            "      current browser session. Please rerun this cell to enable.\n",
                            "      </output>\n",
                            "      <script>// Copyright 2017 Google LLC\n",
                            "//\n",
                            "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
                            "// you may not use this file except in compliance with the License.\n",
                            "// You may obtain a copy of the License at\n",
                            "//\n",
                            "//      http://www.apache.org/licenses/LICENSE-2.0\n",
                            "//\n",
                            "// Unless required by applicable law or agreed to in writing, software\n",
                            "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
                            "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
                            "// See the License for the specific language governing permissions and\n",
                            "// limitations under the License.\n",
                            "\n",
                            "/**\n",
                            " * @fileoverview Helpers for google.colab Python module.\n",
                            " */\n",
                            "(function(scope) {\n",
                            "function span(text, styleAttributes = {}) {\n",
                            "  const element = document.createElement('span');\n",
                            "  element.textContent = text;\n",
                            "  for (const key of Object.keys(styleAttributes)) {\n",
                            "    element.style[key] = styleAttributes[key];\n",
                            "  }\n",
                            "  return element;\n",
                            "}\n",
                            "\n",
                            "// Max number of bytes which will be uploaded at a time.\n",
                            "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
                            "\n",
                            "function _uploadFiles(inputId, outputId) {\n",
                            "  const steps = uploadFilesStep(inputId, outputId);\n",
                            "  const outputElement = document.getElementById(outputId);\n",
                            "  // Cache steps on the outputElement to make it available for the next call\n",
                            "  // to uploadFilesContinue from Python.\n",
                            "  outputElement.steps = steps;\n",
                            "\n",
                            "  return _uploadFilesContinue(outputId);\n",
                            "}\n",
                            "\n",
                            "// This is roughly an async generator (not supported in the browser yet),\n",
                            "// where there are multiple asynchronous steps and the Python side is going\n",
                            "// to poll for completion of each step.\n",
                            "// This uses a Promise to block the python side on completion of each step,\n",
                            "// then passes the result of the previous step as the input to the next step.\n",
                            "function _uploadFilesContinue(outputId) {\n",
                            "  const outputElement = document.getElementById(outputId);\n",
                            "  const steps = outputElement.steps;\n",
                            "\n",
                            "  const next = steps.next(outputElement.lastPromiseValue);\n",
                            "  return Promise.resolve(next.value.promise).then((value) => {\n",
                            "    // Cache the last promise value to make it available to the next\n",
                            "    // step of the generator.\n",
                            "    outputElement.lastPromiseValue = value;\n",
                            "    return next.value.response;\n",
                            "  });\n",
                            "}\n",
                            "\n",
                            "/**\n",
                            " * Generator function which is called between each async step of the upload\n",
                            " * process.\n",
                            " * @param {string} inputId Element ID of the input file picker element.\n",
                            " * @param {string} outputId Element ID of the output display.\n",
                            " * @return {!Iterable<!Object>} Iterable of next steps.\n",
                            " */\n",
                            "function* uploadFilesStep(inputId, outputId) {\n",
                            "  const inputElement = document.getElementById(inputId);\n",
                            "  inputElement.disabled = false;\n",
                            "\n",
                            "  const outputElement = document.getElementById(outputId);\n",
                            "  outputElement.innerHTML = '';\n",
                            "\n",
                            "  const pickedPromise = new Promise((resolve) => {\n",
                            "    inputElement.addEventListener('change', (e) => {\n",
                            "      resolve(e.target.files);\n",
                            "    });\n",
                            "  });\n",
                            "\n",
                            "  const cancel = document.createElement('button');\n",
                            "  inputElement.parentElement.appendChild(cancel);\n",
                            "  cancel.textContent = 'Cancel upload';\n",
                            "  const cancelPromise = new Promise((resolve) => {\n",
                            "    cancel.onclick = () => {\n",
                            "      resolve(null);\n",
                            "    };\n",
                            "  });\n",
                            "\n",
                            "  // Wait for the user to pick the files.\n",
                            "  const files = yield {\n",
                            "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
                            "    response: {\n",
                            "      action: 'starting',\n",
                            "    }\n",
                            "  };\n",
                            "\n",
                            "  cancel.remove();\n",
                            "\n",
                            "  // Disable the input element since further picks are not allowed.\n",
                            "  inputElement.disabled = true;\n",
                            "\n",
                            "  if (!files) {\n",
                            "    return {\n",
                            "      response: {\n",
                            "        action: 'complete',\n",
                            "      }\n",
                            "    };\n",
                            "  }\n",
                            "\n",
                            "  for (const file of files) {\n",
                            "    const li = document.createElement('li');\n",
                            "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
                            "    li.append(span(\n",
                            "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
                            "        `last modified: ${\n",
                            "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
                            "                                    'n/a'} - `));\n",
                            "    const percent = span('0% done');\n",
                            "    li.appendChild(percent);\n",
                            "\n",
                            "    outputElement.appendChild(li);\n",
                            "\n",
                            "    const fileDataPromise = new Promise((resolve) => {\n",
                            "      const reader = new FileReader();\n",
                            "      reader.onload = (e) => {\n",
                            "        resolve(e.target.result);\n",
                            "      };\n",
                            "      reader.readAsArrayBuffer(file);\n",
                            "    });\n",
                            "    // Wait for the data to be ready.\n",
                            "    let fileData = yield {\n",
                            "      promise: fileDataPromise,\n",
                            "      response: {\n",
                            "        action: 'continue',\n",
                            "      }\n",
                            "    };\n",
                            "\n",
                            "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
                            "    let position = 0;\n",
                            "    do {\n",
                            "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
                            "      const chunk = new Uint8Array(fileData, position, length);\n",
                            "      position += length;\n",
                            "\n",
                            "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
                            "      yield {\n",
                            "        response: {\n",
                            "          action: 'append',\n",
                            "          file: file.name,\n",
                            "          data: base64,\n",
                            "        },\n",
                            "      };\n",
                            "\n",
                            "      let percentDone = fileData.byteLength === 0 ?\n",
                            "          100 :\n",
                            "          Math.round((position / fileData.byteLength) * 100);\n",
                            "      percent.textContent = `${percentDone}% done`;\n",
                            "\n",
                            "    } while (position < fileData.byteLength);\n",
                            "  }\n",
                            "\n",
                            "  // All done.\n",
                            "  yield {\n",
                            "    response: {\n",
                            "      action: 'complete',\n",
                            "    }\n",
                            "  };\n",
                            "}\n",
                            "\n",
                            "scope.google = scope.google || {};\n",
                            "scope.google.colab = scope.google.colab || {};\n",
                            "scope.google.colab._files = {\n",
                            "  _uploadFiles,\n",
                            "  _uploadFilesContinue,\n",
                            "};\n",
                            "})(self);\n",
                            "</script> "
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/ipython-input-3169010547.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Move files to datasets folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     67\u001b[0m   \"\"\"\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    162\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    163\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "# 4. Upload dataset files\n",
                "from google.colab import files\n",
                "import os\n",
                "\n",
                "print(\"Upload train.jsonl and golden_test_set.jsonl from your datasets/ folder\")\n",
                "\n",
                "os.makedirs('datasets', exist_ok=True)\n",
                "uploaded = files.upload()\n",
                "\n",
                "# Move files to datasets folder\n",
                "for filename in uploaded.keys():\n",
                "    !mv {filename} datasets/{filename}\n",
                "\n",
                "print(\"\\n‚úì Dataset files uploaded\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Import libraries\n",
                "import torch\n",
                "from transformers import (\n",
                "    AutoModelForCausalLM,\n",
                "    AutoTokenizer,\n",
                "    BitsAndBytesConfig,\n",
                "    TrainingArguments\n",
                ")\n",
                "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
                "from datasets import load_dataset\n",
                "from trl import SFTTrainer\n",
                "\n",
                "print(\"‚úì Libraries imported\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Load model with 4-bit quantization\n",
                "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
                "\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_use_double_quant=True,\n",
                ")\n",
                "\n",
                "print(f\"Loading {model_name}...\")\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True,\n",
                "    token=HF_TOKEN\n",
                ")\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(\n",
                "    model_name,\n",
                "    trust_remote_code=True,\n",
                "    token=HF_TOKEN\n",
                ")\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.padding_side = \"right\"\n",
                "\n",
                "model = prepare_model_for_kbit_training(model)\n",
                "\n",
                "print(\"‚úì Model loaded with 4-bit quantization\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. Configure LoRA\n",
                "lora_config = LoraConfig(\n",
                "    r=16,\n",
                "    lora_alpha=32,\n",
                "    target_modules=[\n",
                "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
                "    ],\n",
                "    lora_dropout=0.05,\n",
                "    bias=\"none\",\n",
                "    task_type=\"CAUSAL_LM\"\n",
                ")\n",
                "\n",
                "print(\"‚úì LoRA configured\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8. Load dataset\n",
                "dataset = load_dataset('json', data_files={\n",
                "    'train': 'datasets/train.jsonl',\n",
                "    'test': 'datasets/golden_test_set.jsonl'\n",
                "})\n",
                "\n",
                "print(f\"‚úì Loaded {len(dataset['train'])} training samples\")\n",
                "print(f\"‚úì Loaded {len(dataset['test'])} test samples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9. Format instruction function\n",
                "def format_instruction(sample):\n",
                "    return f\"\"\"### Instruction:\n",
                "{sample['instruction']}\n",
                "\n",
                "### Response:\n",
                "{sample['output']}\"\"\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 10. Configure training\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./lora_adapters\",\n",
                "    num_train_epochs=3,\n",
                "    per_device_train_batch_size=4,\n",
                "    per_device_eval_batch_size=4,\n",
                "    gradient_accumulation_steps=4,\n",
                "    learning_rate=2e-4,\n",
                "    fp16=True,\n",
                "    logging_steps=10,\n",
                "    save_strategy=\"epoch\",\n",
                "    evaluation_strategy=\"epoch\",\n",
                "    warmup_steps=50,\n",
                "    lr_scheduler_type=\"cosine\",\n",
                "    optim=\"paged_adamw_8bit\",\n",
                "    report_to=\"none\"\n",
                ")\n",
                "\n",
                "print(\"‚úì Training configured\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 11. Create trainer\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    train_dataset=dataset['train'],\n",
                "    eval_dataset=dataset['test'],\n",
                "    peft_config=lora_config,\n",
                "    dataset_text_field=\"text\",\n",
                "    formatting_func=format_instruction,\n",
                "    tokenizer=tokenizer,\n",
                "    args=training_args,\n",
                "    max_seq_length=512,\n",
                ")\n",
                "\n",
                "print(\"‚úì Trainer created\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 12. START TRAINING! üöÄ\n",
                "print(\"=\"*60)\n",
                "print(\"TRAINING STARTED\")\n",
                "print(\"=\"*60)\n",
                "print(\"This will take ~1.5-2 hours on T4 GPU\")\n",
                "print(\"You can close this tab - training will continue!\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 13. Save model\n",
                "trainer.save_model(\"./lora_adapters\")\n",
                "print(\"\\n‚úì Model saved to lora_adapters/\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 14. Test the model\n",
                "test_question = \"What was Uber's revenue in 2024?\"\n",
                "\n",
                "prompt = f\"\"\"### Instruction:\n",
                "{test_question}\n",
                "\n",
                "### Response:\n",
                "\"\"\"\n",
                "\n",
                "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
                "\n",
                "with torch.no_grad():\n",
                "    outputs = model.generate(\n",
                "        **inputs,\n",
                "        max_new_tokens=150,\n",
                "        temperature=0.7,\n",
                "        do_sample=True\n",
                "    )\n",
                "\n",
                "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "print(\"\\nTest Question:\", test_question)\n",
                "print(\"\\nModel Answer:\")\n",
                "print(response.split(\"### Response:\")[1].strip())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 15. Download the trained model\n",
                "!zip -r lora_adapters.zip lora_adapters/\n",
                "\n",
                "from google.colab import files\n",
                "files.download('lora_adapters.zip')\n",
                "\n",
                "print(\"\\n‚úì Download started!\")\n",
                "print(\"\\nExtract this on your local PC to: models/lora_adapters/\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéâ Done!\n",
                "\n",
                "### Next Steps:\n",
                "1. Download `lora_adapters.zip` (should start automatically)\n",
                "2. Extract to your local PC: `Mini Project 1/models/lora_adapters/`\n",
                "3. Run evaluation: `python utils/evaluate_systems.py`\n",
                "4. Launch web UI: `python web_ui.py`\n",
                "\n",
                "**Training Complete!** ‚úÖ"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
