{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üéì The Intern: Fine-Tuned Model Inference\n",
                "\n",
                "This notebook demonstrates how to load and use \"The Intern\" - our Llama-3 8B model fine-tuned on Uber's 2024 Annual Report.\n",
                "\n",
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
                "from peft import PeftModel\n",
                "import os\n",
                "\n",
                "# Load environment variables\n",
                "try:\n",
                "    with open('../.env', 'r') as f:\n",
                "        for line in f:\n",
                "            line = line.strip()\n",
                "            if line and not line.startswith('#') and 'export' in line:\n",
                "                parts = line.replace('export ', '').split('=', 1)\n",
                "                if len(parts) == 2:\n",
                "                    os.environ[parts[0].strip()] = parts[1].strip().strip('\"')\n",
                "except FileNotFoundError:\n",
                "    print(\"Note: .env file not found in parent directory\")\n",
                "\n",
                "print(\"Setup complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Model\n",
                "\n",
                "We load the base model in 4-bit quantization and attach the LoRA adapters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
                "adapter_path = \"../models/lora_adapters\"\n",
                "\n",
                "print(f\"Loading {model_name} with adapters from {adapter_path}...\")\n",
                "\n",
                "# 1. Load Base Model (Quantized)\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                ")\n",
                "\n",
                "base_model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    token=os.getenv('HF_TOKEN')\n",
                ")\n",
                "\n",
                "# 2. Load Adapters\n",
                "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name, token=os.getenv('HF_TOKEN'))\n",
                "\n",
                "print(\"‚úì Model loaded successfully\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def query_intern(question: str, max_new_tokens=200):\n",
                "    \"\"\"Generate answer from The Intern\"\"\"\n",
                "    prompt = f\"\"\"### Instruction:\n",
                "{question}\n",
                "\n",
                "### Response:\n",
                "\"\"\"\n",
                "    \n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=max_new_tokens,\n",
                "            temperature=0.7,\n",
                "            do_sample=True,\n",
                "            top_p=0.9\n",
                "        )\n",
                "    \n",
                "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    if \"### Response:\" in response:\n",
                "        response = response.split(\"### Response:\")[1].strip()\n",
                "        \n",
                "    return response"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Inference Testing\n",
                "\n",
                "Let's test the model on specific questions from the annual report."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "questions = [\n",
                "    \"What was Uber's total revenue in 2024?\",\n",
                "    \"What are the main risk factors mentioned?\",\n",
                "    \"How does Uber describe its competitive position?\",\n",
                "    \"What messages did the CEO share with shareholders?\"\n",
                "]\n",
                "\n",
                "for q in questions:\n",
                "    print(f\"\\n‚ùì Question: {q}\")\n",
                "    answer = query_intern(q)\n",
                "    print(f\"üí° Answer: {answer}\\n\")\n",
                "    print(\"-\"*60)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}